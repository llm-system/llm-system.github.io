---
title: 深入大模型系统札记：第7章 符号推理方法
description: "解码搜索策略（思维链、自一致性、思维树）是产业级推理任务的必由之路；策略内化通过苏格拉底产婆术让模型在自回归过程中自主完成搜索与验证。"
draft: false
---

> **本章概要**：本章的核心是两层递进的洞察。**第一层：解码搜索策略是产业级任务的必由之路**，默认解码在复杂推理任务上系统性失效，必须引入显式搜索策略（思维链、自一致性、思维树）才能处理代码生成、数学推理、多步规划等产业级任务。**第二层：将搜索策略内化到模型的自回归思考过程中**，通过苏格拉底产婆术，让模型在长思维链中学会自问自答，实现类似解码搜索的结构性探索，减少对显式解码编排的依赖。

---

大语言模型真的会"推理"吗？

答案是：**它不会像人类那样推理，但它可以被引导去模拟推理的过程**。思维链、思维树这些技术，本质上是把人类的推理结构"教"给模型，让它按照这个结构生成文本。这不是"真正的推理"，但你要问自己：在工程场景下，"模拟推理"和"真正推理"有区别吗？如果输出正确、可靠、可复现，区别并不重要。

第六章讨论了如何通过提示工程构建任务的语义层，本章进一步探讨如何在语义层上进行结构化推理。本章的核心是两层递进的洞察：

**第一层：解码搜索策略是产业级任务的必由之路**。大语言模型的默认解码策略（贪婪解码、采样）在开放式对话中表现良好，但在需要严密逻辑的复杂任务上系统性失效。要让大语言模型真正处理产业级任务，代码生成、数学推理、多步规划、复杂决策，必须引入显式的解码搜索策略。

**第二层：将搜索策略内化到模型的自回归思考过程中**。显式解码策略有显著的成本和局限：推理开销大、需要外部系统编排、难以规模化部署。更优雅的解决方案是通过训练将这些策略内化到模型的"思考"过程中，让模型在生成答案之前，先在内部完成搜索、验证、修正，减少对显式解码编排的依赖。

---

## 第一部分：解码搜索策略

为什么解码搜索策略是产业级任务的必由之路？这需要从"生成即搜索"的视角来理解。

### 一、生成即搜索

LLM 的基础任务是预测下一个词元，但复杂推理需要多步规划和全局一致性。理解这种能力跨越的统一框架是**状态空间搜索**：**状态**是当前已生成的词元序列，**动作**是从词汇表中选择下一个词元，**状态转移**是将新词元拼接到序列末尾。整个生成过程在时间维度上展开为一棵**状态空间搜索树**，根节点是初始提示，每条边代表一次词元选择，从根到任意节点的路径对应一种可能的生成结果。

对于推理任务，搜索面临严峻挑战：**巨大的分支因子**（每个节点从数万候选词元中选择）、**不可逆的决策**（错误动作对后续路径产生不可逆影响）、**局部最优陷阱**（标准解码策略无法保证全局最优）。数学计算、代码生成、逻辑推理对路径有刚性约束，每一步都严格影响后续步骤的合法性。**推理的本质不再是生成流畅文本，而是在巨大的可能性空间中规划出一条满足约束的"合法路径"**。

然而，贪婪解码、温度采样等标准解码策略在推理任务中频繁失败。这些策略的共同机制是在每一步仅依据局部概率分布决策，而缺乏对整条路径最终是否合法的评估，它们只关心"下一步往哪里走最自然"，而从未评估"沿着这条路走最终能否到达终点"。典型失效模式包括：**偏差累积**（初期微小偏差导致后续逻辑链断裂）、**前瞻缺失**（无法为全局目标选择当前概率不高但关键的"铺垫性"步骤）、**验证缺位**（仅生成一条路径且不验证其合法性）。LLM 之所以"预测精准，推理拙劣"，是因为默认策略与推理任务本质要求之间存在**结构性错配**。

### 二、测试时缩放

在保持模型结构不变的情况下，能否仅通过改变推理时的行为来提升推理能力？近期研究给出了肯定答案。

大语言模型能力提升经历了三个缩放阶段：**预训练缩放**通过增加参数和数据提升基础能力，**后训练缩放**通过对齐和微调提升任务适配能力，**测试时缩放**则通过增加推理时的计算量提升复杂任务的完成质量。核心发现是：单纯在推理阶段延长输出长度，就能带来显著性能提升。

测试时缩放的基础来自生成式架构的**工作记忆外化**（第四章）。仅解码器架构将工作记忆从模型内部状态转移到显式的生成内容中，模型可以把中间推理步骤"写"到输出序列中，再通过自注意力机制"读"回来。这意味着：**更多词元预算等于更大工作记忆容量**。输出长度不再仅仅是"答案的长度"，而是模型可用于思考的"草稿纸大小"。

从搜索视角看，每生成一个词元就是一次状态转移，输出长度直接定义了"行动预算"。对于需要三步逻辑的数学题，若限制预算为两步，无论单步预测能力如何，都无法找到答案。这引出关键结论：**解码生成质量决定了任务完成质量**，如果生成的中间步骤逻辑混乱，再多的词元预算也只是在积累"垃圾"。

### 三、思维链：单路径深度搜索

**思维链提示**（Chain-of-Thought，简称 CoT）是测试时缩放的典型实现。通过简单的自然语言指令（如"让我们一步步思考"），思维链激活了模型在状态空间搜索树中的搜索轨迹，促使模型进行更多次状态转移，形成一条更长、更深的路径。

在贪婪解码下，思维链的执行等同于**单路径深度优先搜索**，模型从初始状态出发，每一步选择局部概率最高的词元，在搜索树中形成一条不可回溯的单一路径。思维链本质上不是新的搜索算法，而是对底层贪婪解码机制的"结构化引导"。在 DeepSeek-R1 等先进模型中，逐步推理能力已部分内化，无须显式提示，模型也能根据任务复杂性自发生成包含中间步骤的推理过程。

然而思维链对单路径深度优先搜索的依赖使其存在固有缺陷：**结构脆弱性**（早期错误导致"一步错，步步错"的链式失败）、**解非最优性**（无法比较多条路径）、**搜索效率低**（易陷入局部陷阱）。

### 四、自一致性：多路径并行

**自一致性机制**（Self-Consistency）通过"多路径采样加投票"显著提高推理鲁棒性。核心流程：基于相同提示生成多条不同的推理路径，对每条路径的最终答案进行提取，通过多数投票选择出现次数最多的答案。

这一策略的精髓在于：承认任何单条推理路径都可能出错，但相信正确答案会在多次独立"思考"中反复出现。它通过路径多样性对冲单次推理的偶然性，用答案冗余度换取结果鲁棒性。自一致性巧妙地将 LLM 生成的内在不确定性从缺陷转化为优势，不再苛求单一路径完美无缺，只要最终"共识解"正确即可。

然而自一致性也有核心局限：**路径间信息隔离**（所有路径独立生成，彼此无信息交流）、**探索不可控**（完全依赖随机采样，缺乏主动规划）、**评估粒度粗糙**（投票机制只关注最终答案，忽略推理过程质量）。

### 五、思维树：语义层搜索

**思维树**（Tree of Thoughts，简称 ToT）将搜索优化从词元层提升到语义层。传统解码的评估标准是语言概率（"这个词接在前面是否流畅"），思维树的评估标准是**语义层的业务规则**（"这个状态是否满足约束"）。

回顾第六章，语义层是业务概念与底层数据之间的抽象。思维树的核心洞察是：推理任务的合法性不应用语言概率评估，而应用语义层定义的规则评估。数独的约束（每行每列数字不重复）、24 点的数学约束、代码的编译测试，这些都是语义层的"数字孪生规则"。

思维树的工作流程体现了这种提升：**节点**不再是词元序列，而是语义层状态（数独的当前填充状态、代码的当前实现）；**边**是语义层状态转移；**状态评估器**用业务规则而非语言概率进行评估；**搜索算法**（如蒙特卡罗树搜索）在语义层进行剪枝和规划。思维树将大语言模型的生成能力与语义层的规则系统相结合，大语言模型负责生成候选状态转移，语义层规则负责评估和筛选。

至此，第一部分的结论是清晰的：**解码搜索策略是产业级任务的必由之路**。从思维链到自一致性到思维树，搜索能力不断增强，但都依赖外部系统的显式编排。

---

## 第二部分：策略内化

显式解码策略有显著的成本和局限：每次推理都需要多次采样或搜索，推理开销成倍增加；需要外部系统协调解码过程，架构复杂度上升；难以在端侧或资源受限环境中部署。更优雅的解决方案是：**将搜索策略内化到模型的自回归思考过程中**，让模型学会在生成答案之前，先在"思考"阶段自主执行搜索、验证、修正。

### 六、从外部编排到内部思考

前述思维链、自一致性、思维树都依赖外部机制控制搜索行为。能否让模型将这些策略彻底内化？

搜索策略的控制权正在从外部注入转移到模型内部自主执行。**外部提示控制**是起点，"让我们一步步思考"激活思维链，提示的微小差异可能导致模型走向截然不同的路径。这本质上是人类在外部"编排"模型的解码过程。

关键转折是**策略内化**：让模型在自回归生成过程中自主执行搜索策略，而不是依赖外部编排。OpenAI o1 是这一范式的开创者，首次证明模型可以通过引入结构化的"思考"阶段在内部完成搜索与验证 (OpenAI, 2024)；DeepSeek-R1 随后以开源方式跟进，通过强化学习训练让模型学会"先思考，后回答"的结构化模式（用"开始思考"和"结束思考"标记界定）。在"思考"阶段，模型可以自主进行探索、验证、回溯、修正，这些原本需要外部搜索算法完成的工作，现在在模型内部的自回归生成中完成。

### 七、苏格拉底产婆术

如何让模型学会在"思考"阶段自主执行搜索策略？答案是**苏格拉底产婆术**，让模型在长思维链中学会自问自答。

苏格拉底产婆术的原意是：教师不直接给出答案，而是通过连续追问引导学生自己"生出"知识。在策略内化的语境下，这意味着：**模型在"思考"阶段自己向自己提问，然后自己回答，通过这种自问自答的循环实现类似解码搜索的结构性探索**。

具体而言，当模型面对复杂问题时，内化了苏格拉底产婆术的思考过程可能是这样的："这个问题的关键约束是什么？""让我先检查第一个条件……""等等，这里有矛盾，之前的假设可能有问题。""如果换一个方向呢？""让我验证一下这个中间结果是否正确……"这些自问自答的片段，本质上是在思考过程中执行搜索策略：提问是"分支"，回答是"探索"，发现矛盾是"剪枝"，换方向是"回溯"。

**过程奖励建模**是训练模型学会苏格拉底产婆术的关键方法。传统的结果奖励只关注最终答案的正确性，奖励信号过于稀疏。过程奖励则对思考过程的每一步评分：逻辑清晰的自问自答获得高奖励，混乱跳跃的推理被惩罚。通过强化学习，模型逐渐学会在"思考"阶段生成结构化的自问自答序列，这些原本需要显式搜索算法（自一致性、思维树）完成的功能，现在被内化为模型的"内心独白"。

这也解释了为什么**代码语料对模型的思维链能力有显著帮助**，代码本身就是演绎法的语言。演绎推理的特点是从前提到结论的严格推导，每一步都是确定性的、可验证的；代码正是演绎法的形式化表达，每一行都是一个确定性的状态转移，程序执行就是演绎推理的过程。代码语料中天然包含结构化的自问自答模式：注释解释"为什么这样做"；条件分支是"如果……那么……"的结构化探索；函数分解是"这个问题可以拆成哪些子问题"；错误处理是"如果这里出问题了怎么办"；断言和日志是"让我检查一下这个中间状态"。代码语料是苏格拉底产婆术的天然训练素材，程序员在写代码时不断进行的演绎推理和自问自答，被模型学习后迁移到了通用推理任务中。

### 八、策略涌现

当策略内化达到一定程度，模型会表现出**策略涌现**，超越训练数据模式匹配的能力。

涌现的典型表现包括：**动态策略组合**，模型根据任务中间状态灵活切换广度优先搜索与深度优先搜索，而不是机械地执行固定策略；**元认知与自我修正**，思维链中出现"我们先前的假设似乎有问题，让我们重新检查"这类反身性语言片段；**问题分解与重组**，面对复杂问题时自发将其分解为子问题，解决后再整合。

策略涌现依赖三个关键条件：巨大的模型规模与上下文长度提供了足够的"思考空间"；海量结构多样的训练样本让模型见过各种推理模式；以"过程"为核心的精细化训练信号（过程奖励）引导模型学会好的思考习惯而非仅仅模仿答案。

策略涌现的意义在于：**减少对显式解码策略的依赖**。当模型能够在内部自主执行搜索、验证、修正时，外部系统只需要提供足够的"思考预算"（词元数量），而不需要复杂的搜索编排。这大大简化了产业级部署的架构复杂度。

---

## 小结

本章的核心是两层递进的洞察。

**第一层：解码搜索策略是产业级任务的必由之路**。大语言模型的默认解码策略在复杂推理任务上系统性失效，必须引入显式搜索策略。从"生成即搜索"的视角看，推理的本质是在状态空间搜索树中寻找满足约束的合法路径。思维链实现单路径深度搜索，自一致性通过多路径采样提升鲁棒性，思维树将搜索优化从词元层提升到语义层。这些策略是处理代码生成、数学推理、多步规划等产业级任务的必由之路。

**第二层：将搜索策略内化到模型的自回归思考过程中**。显式解码策略有显著的成本和局限，更优雅的解决方案是通过训练将策略内化。苏格拉底产婆术是核心机制：让模型在长思维链中学会自问自答，通过这种内心独白实现类似解码搜索的结构性探索。过程奖励建模是训练方法：对思考过程的每一步评分，引导模型学会生成结构化的自问自答序列。当策略内化达到一定程度，模型会表现出策略涌现，动态策略组合、元认知与自我修正，从而减少对显式解码编排的依赖。

当模型不仅能在语言中"思考"，更能将这种思考转化为与世界交互的"行动"时，它便是一个拥有目标、具备感知与执行能力的语言智能体，这正是下一章的主题。

最后提一个趋势判断：**DeepSeek-R1 证明了"策略内化"这条路是走得通的**。再过几年，你回头看今天的思维链和思维树，很可能会觉得它们就像当年的人工特征工程一样，有用，但终将被更优雅的方案取代。

但这并不意味着提示工程会消亡。**提示工程的角色正在从"教模型如何思考"转变为"给模型提供正确的上下文"**。当模型自身已经内化了搜索与验证策略，外部系统不再需要编排思考过程，但仍然需要为模型准备高质量的任务上下文、领域知识和行动空间——这正是下一章智能体系统和第九章上下文工程要解决的问题。理解这个方向，你才能在技术演进中保持判断力。

## 引用本章

```bibtex
@book{baillmsystem,
  title     = {深入大模型系统：提示工程、符号推理与智能体实践},
  author    = {Bai, Yu},
  publisher = {人民邮电出版社},
  year      = {2025},
  isbn      = {978-7-115-68707-4}
}
```

## 文献列表

- **Chain-of-Thought: 思维链提示激发大语言模型的推理能力**
  Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Wei, Jason et al.
  [原文](https://arxiv.org/abs/2201.11903)
- **Zero-shot CoT: 大语言模型是零样本推理者**
  Large Language Models are Zero-Shot Reasoners. Kojima, Takeshi et al.
  [原文](https://arxiv.org/abs/2205.11916)
- **Self-Consistency: 自一致性改进语言模型的思维链推理**
  Self-Consistency Improves Chain of Thought Reasoning in Language Models. Wang, Xuezhi et al.
  [原文](https://arxiv.org/abs/2203.11171)
- **Tree of Thoughts: 使用大语言模型进行深思熟虑的问题求解**
  Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Yao, Shunyu et al.
  [原文](https://arxiv.org/abs/2305.10601)
- **Let's Verify Step by Step: 过程监督的数学推理**
  Let's Verify Step by Step. Lightman, Hunter et al.
  [原文](https://arxiv.org/abs/2305.20050)
- **OpenAI o1: 学会推理的大语言模型**
  Learning to Reason with LLMs. OpenAI. 2024.
  [原文](https://openai.com/index/learning-to-reason-with-llms/)
- **DeepSeek-R1: 通过强化学习激励 LLM 的推理能力**
  DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. DeepSeek-AI.
  [原文](https://arxiv.org/abs/2501.12948)
- **Scaling Test-Time Compute: 测试时计算的扩展定律**
  Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. Snell, Charlie et al.
  [原文](https://arxiv.org/abs/2408.03314)
- **Reasoning via Planning: 通过规划进行推理**
  Reasoning with Language Model is Planning with World Model. Hao, Shibo et al.
  [原文](https://arxiv.org/abs/2305.14992)
- **MCTS for LLM: 蒙特卡罗树搜索用于 LLM 推理**
  Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation. Ding, Ruomeng et al.
  [原文](https://arxiv.org/abs/2311.04254)
- **Process Reward Models: 过程奖励模型**
  Solving Math Word Problems with Process- and Outcome-Based Feedback. Uesato, Jonathan et al.
  [原文](https://arxiv.org/abs/2211.14275)
